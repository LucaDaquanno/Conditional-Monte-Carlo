{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c859ce61",
   "metadata": {},
   "source": [
    "# Flexible probabilities for scenario analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd45759",
   "metadata": {},
   "source": [
    "__Importing libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355ed263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import scipy.stats as sts\n",
    "from scipy.stats import norm,chi2,t,lognorm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import statistics\n",
    "import time\n",
    "import plotly as plty\n",
    "import scipy.optimize as spopt\n",
    "import datetime\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from arch import arch_model\n",
    "from scipy.stats import gaussian_kde\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a4fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=1\n",
    "if user ==1:\n",
    "    path = \"/Users/lucadaquanno/Desktop/Documents/CIOS.Analyse/Return_forecasting/Entropy_pooling_python/\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9d0b8",
   "metadata": {},
   "source": [
    "## Connecting the API and send Time series requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a8d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_flex(list_of_ISIN, start_date, end_date, **kwargs):\n",
    "    list_of_dcts=[]\n",
    "    for e in list_of_ISIN:\n",
    "        d={\"code\": e, \"code_type\": \"isin\"}\n",
    "        list_of_dcts.append(d)\n",
    "    dct_body={\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"instruments\": list_of_dcts,\n",
    "        \"convert_prices\": False,\n",
    "        \"use_live_data\": True,\n",
    "        \"extend_timeseries_in_db\": False,\n",
    "        \"extend_investment_universe\": False,\n",
    "        \"source\": \"merged\"\n",
    "    }\n",
    "    dct_body.update(kwargs)\n",
    "    body = json.dumps(dct_body)\n",
    "    r = requests.post(\"https://data.acp-cios.fincite.net/api/v1/timeseries/\", data=body,\n",
    "                         headers = {\n",
    "                             'content-type':'application/json',\n",
    "                             'authorization':'Bearer L0hxZj2udrAgY1QxqW1rG5HkshYR0EY8AU9QMtDM'})\n",
    "    return json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0af43b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "isin=[\"US78378X1072\",\"US2605661048\",\"IE0031719473\",\"US4642876894\",\"CH0012138530\"]\n",
    "start_date='2000-12-31'\n",
    "end_date='2022-12-31'\n",
    "response=time_series_flex(isin, start_date, end_date)\n",
    "response_list=response['response']['instruments']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09174b3",
   "metadata": {},
   "source": [
    "## Transforming the Response into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4fb558",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "for k in response_list:\n",
    "    response_dict=k['timeseries']\n",
    "    dates_index = list(map(itemgetter('date'), response_dict))\n",
    "    dates_index=[datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates_index]\n",
    "    close_prices=list(map(itemgetter('close_price'), response_dict))\n",
    "    prices=pd.DataFrame(close_prices,dates_index)\n",
    "    #x=np.log(prices).diff().dropna()\n",
    "    #x=x.resample('M').sum()\n",
    "    df=pd.concat([df,prices],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014d40d",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "525c90ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily data\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_excel(path+\"dsws_timeseries.xlsx\", parse_dates = [\"date\"], index_col=(\"date\") )\n",
    "start_date = df.index.min()\n",
    "end_date  = df.index.max() #  last available date\n",
    "df = df[start_date : end_date]\n",
    "bdate = pd.bdate_range(start_date, end_date) # get only business day\n",
    "x = df.copy()\n",
    "for i in bdate:\n",
    "    if (i in x.index) == False: #checking missing values\n",
    "        x.loc[i,:] = np.nan\n",
    "x = x.sort_index(ascending=True)\n",
    "spline = False\n",
    "if spline:\n",
    "    x = x.interpolate(method = \"cubic\")\n",
    "else:\n",
    "    x = x.fillna(method = \"ffill\")\n",
    "name = ['SP500','DOW_Jones','Fixed_Income','Russell3000','Credit_Suisse']\n",
    "x.columns = name\n",
    "dates=x.index\n",
    "x=x.pct_change().dropna()\n",
    "#x=np.log(x).diff().dropna()\n",
    "Time_scaling={'daily':'d','monthly':'m','yearly':'y'}\n",
    "data_frequency='daily'\n",
    "scaling_factor=Time_scaling[data_frequency]\n",
    "if scaling_factor=='m':\n",
    "    x=(1+x).resample('M').prod()-1\n",
    "    print('monthly data')\n",
    "elif scaling_factor=='y':\n",
    "    x=(1+x).resample('Y').prod()-1\n",
    "    print('yearly data')\n",
    "else:\n",
    "    print('daily data')\n",
    "\n",
    "#x.index=np.arange(0,len(x))\n",
    "#print(x.loc[x.index[0]:x.index[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61ff37",
   "metadata": {},
   "source": [
    "### Defining our prior: time-conditioned probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240487d",
   "metadata": {},
   "source": [
    "Tipically we need to rely more on recent scenarios and possibly on additional information on the market. <br>\n",
    "This leads to alternative specifications of probabilities based on the notions of time conditioning and state conditioning respectively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7c380",
   "metadata": {},
   "source": [
    "In the time conditioning approach the relative weight of each scenario depends on the time elapsed. <br>\n",
    "1\\. $ p_t|\\tau_{HL}$ := $pe$ $^{-\\frac{ln(2)}{\\tau_{HL}}|t - T|}$ <br>\n",
    "2\\. $p$ := 1/ $ \\sum_{t}^{} e^{-\\frac{ln(2)}{\\tau_{HL}}|t - T|}$  <br>\n",
    "\n",
    "* $\\tau_{HL}$ can be interpreted as the  time required for the probability of a scenario to decrease to half of its maximum value in $T$  <br>\n",
    "* the lower is $\\tau_{HL}$ the higher is the decay rate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a751b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exp_Decay_prob(X,T_date,Tau_date,data_freq=scaling_factor):\n",
    "    '''X is the timeseries of risk_drivers\n",
    "    T_date is the latest observation's date\n",
    "    Tau_date is the date for the half life parameter\n",
    "    this function return a series of time-decaying probabilities'''\n",
    "    if data_freq=='d':\n",
    "        X=X.loc[:T_date]\n",
    "        Tau_integer=X.loc[:Tau_date].shape[0] # associating an integer to the Tau_date\n",
    "        T_integer=X.shape[0]                 # associating an integer to the T date\n",
    "        exponent=[-(np.log(2)/Tau_integer)*abs((t-T_integer))for t in np.arange(0,T_integer)]\n",
    "        P=1/np.sum(np.exp(exponent))\n",
    "        time_conditioned_p=P*np.exp(exponent)\n",
    "        return pd.Series(time_conditioned_p,name='T_cond_prob',index=X.index)\n",
    "    elif data_freq == 'm':\n",
    "        X=X.loc[:T_date]\n",
    "        Tau_integer=X.loc[:Tau_date].shape[0] # associating an integer to the Tau_date\n",
    "        T_integer=X.shape[0]              # associating an integer to the T date\n",
    "        X=X.loc[:T_date]\n",
    "        exponent=[-(np.log(2)/Tau_integer)*abs((t-T_integer))for t in np.arange(0,T_integer)]\n",
    "        P=1/np.sum(np.exp(exponent))\n",
    "        time_conditioned_p=pd.Series(P*np.exp(exponent),name='T_cond_prob',index=X.index)\n",
    "        return time_conditioned_p.resample('M').sum()\n",
    "    else:\n",
    "        X=X.loc[:T_date]\n",
    "        Tau_integer=X.loc[:Tau_date].shape[0] # associating an integer to the Tau_date\n",
    "        T_integer=X.shape[0]              # associating an integer to the T date\n",
    "        X=X.loc[:T_date]\n",
    "        exponent=[-(np.log(2)/Tau_integer)*abs((t-T_integer))for t in np.arange(0,T_integer)]\n",
    "        P=1/np.sum(np.exp(exponent))\n",
    "        time_conditioned_p=pd.Series(P*np.exp(exponent),name='T_cond_prob',index=X.index)\n",
    "        return time_conditioned_p.resample('Y').sum()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115d3271",
   "metadata": {},
   "source": [
    "To express our views on volatility, we may need to consider a restricted dataset (observations - rolling_window) and initialize a prior distribution based on this limited information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63199cd",
   "metadata": {},
   "source": [
    "With Re-scaled data the optimizer works better, we are going to apply a z-score normalization on our original dataset <br>\n",
    "$ \\epsilon= \\frac{x - \\bar{x}}{\\sigma(x)}$\n",
    "* $\\bar{x}$ is the returns sample mean\n",
    "* $\\sigma(x)$ is the returns standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd959c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaling_factor=='d':\n",
    "    wndw=252\n",
    "elif scaling_factor=='m':\n",
    "    wndw=12\n",
    "else:\n",
    "    wndw=2\n",
    "#x_r=x.iloc[0:(len(x)-wndw+1)]\n",
    "x_restricted=x.iloc[wndw-1:]\n",
    "data_sample_mean=x.mean()\n",
    "data_sample_volat=x.std()\n",
    "std_data=(x-data_sample_mean)/data_sample_volat\n",
    "epsilon=std_data.copy()\n",
    "epsilon_restricted=epsilon.iloc[wndw-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e55fda",
   "metadata": {},
   "source": [
    "## Testing the function for the prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431502fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tau_date='2020-01-04'\n",
    "T_date=epsilon_restricted.index[-1]\n",
    "time_cond_prob= Exp_Decay_prob(epsilon_restricted,T_date,tau_date)\n",
    "print(np.sum(time_cond_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de32f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_decay_flag=True\n",
    "if exp_decay_flag:\n",
    "    p_0=time_cond_prob\n",
    "else: #equally weighted probability as a prior\n",
    "    p_0=pd.Series(np.ones(len(epsilon_restricted))*1/len(epsilon_restricted),index=epsilon_restricted.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd964e35",
   "metadata": {},
   "source": [
    "## Defining the user views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c5d1de",
   "metadata": {},
   "source": [
    "The most simple function $v_1(X)$ we can think about, is the function that maps our risk drivers $X$ in a portfolio.\n",
    "A function mapping an N-dimensional object to a one-dimensional object.\n",
    "* $V_1(X) :=  Xw $\n",
    "* $\\mathbb{E}_p{\\bigg(V_1(X)\\bigg)} := p'V_1(X) $\n",
    "* $V:=E_{p}{V_1(X)}\\geq v_{*_{1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6baf41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v_1x=epsilon_restricted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad14d45",
   "metadata": {},
   "source": [
    "# Prior Expected values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3af97116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SP500            0.003180\n",
       "DOW_Jones        0.003002\n",
       "Fixed_Income    -0.026547\n",
       "Russell3000      0.002915\n",
       "Credit_Suisse   -0.002746\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_1x.T.dot(p_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c96b0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_frequency=='daily':\n",
    "    scaling_adjustment=252\n",
    "elif data_frequency=='monthly':\n",
    "    scaling_adjustment=22\n",
    "else:\n",
    "    scaling_adjustment=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1656e9",
   "metadata": {},
   "source": [
    "* $v_{*_{1}} \\approx -10\\%$ (yearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68104cf",
   "metadata": {},
   "source": [
    "Suppose we have  a bearish views for our portfolio, we can state our view as follows : <br>\n",
    "* $V:E_{p}{v_1(X)}\\leq v_{*_{1}}$\n",
    "* $\\bigg[p'[Xw]\\bigg] \\leq v_{*_{1}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215d3a5",
   "metadata": {},
   "source": [
    "We can define also a constraint for the portfolio volatility. Let:\n",
    "* $\\Sigma_{x_{t}}$  NXN  semi-positive definite rolling var-cov matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f9fce8",
   "metadata": {},
   "source": [
    "* $v_2(x_{t}) := \\sqrt{w'\\Sigma_{x{t}} w}$ <br>\n",
    "* $V_2(X) := \\bigg[v_2(x_{1}),v_2(x_{2}),...,v_2(x_{t}),...,v_2(x_{T})\\bigg]'$ <br>\n",
    "* $\\mathbb{E}_p{\\bigg(V_2(X)\\bigg)} := p'V_2(X) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb6ffd",
   "metadata": {},
   "source": [
    "\n",
    "we can set the intensity of our view $v_2*$ based on the current level of the volatility for our portfolio. <br>\n",
    "\n",
    "Assuming  a more volatile market, we can state our view as follows:\n",
    "\n",
    "*  $V:=E_{p}\\bigg[{V_2(X)}\\bigg]\\geq v_{*_{2}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4e390",
   "metadata": {},
   "source": [
    "$v_{*_{2}} \\approx 11\\%$ (yearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf1e4c",
   "metadata": {},
   "source": [
    "Defining $V_2(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f51c22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_2x=(epsilon).rolling(window=wndw).std().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc89dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio volatility is SP500            0.012919\n",
      "DOW_Jones        0.012276\n",
      "Fixed_Income     0.001325\n",
      "Russell3000      0.013044\n",
      "Credit_Suisse    0.025418\n",
      "dtype: float64\n",
      "Portfolio volatility with standardiized data is SP500            1.011200\n",
      "DOW_Jones        1.012648\n",
      "Fixed_Income     1.027226\n",
      "Russell3000      1.011427\n",
      "Credit_Suisse    1.020578\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "'Portfolio volatility'\n",
    "print('Portfolio volatility is ' +str((x_restricted).std()))\n",
    "'portfolio with standardized data'\n",
    "print('Portfolio volatility with standardiized data is ' + str(v_1x.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd933fc",
   "metadata": {},
   "source": [
    "##  Entropy minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93105c10",
   "metadata": {},
   "source": [
    "$p^{post}$ = $ argmin_{q} \\ \\Bigg \\{ \\sum_{t=1}^{T}q_t(ln(q_t) - ln(p^{0}_{t})) \\Bigg \\}$ <br>\n",
    "$ Subject \\ to$<br> \n",
    "$Fq\\leq f$ <br> \n",
    "$Hq$ $=$ $h$ <br>\n",
    "We have collected all the inequality constraints in the matrix-vector pair $(F,f)$ and\n",
    "all the equality constraints in the matrix-vector pair $(H,h)$, we do not include the extra-constraint\n",
    "$\\bigg(q>0\\bigg)$ because it will be automatically satisfied. <br>\n",
    " The Lagrangian function reads: <br> <br>\n",
    " $L(q,\\lambda_{1} , \\lambda_{2})$ = $q'(ln(q)-ln(p^{0})) + \\lambda_1' (Fq-f) +  \\lambda_2'(Hq-h)$ <br> <br>\n",
    "   * $\\lambda_1$ is a row  vector with number of inequality constraint = number of rows <br>\n",
    "   * $\\lambda_2$ is a row  vector with number of equality constraint = number of rows <br>\n",
    "   * $F$ is a matrix with K rows (K=number of inequality constraint) and T columns(number of risk drivers' observations) <br>\n",
    "   * $H$ is a matrix with J rows (J=number of equality constraint) and T columns (number of risk drivers' observations) <br>\n",
    "\n",
    " the first order condition for q read: <br> <br>\n",
    " $ 0 = \\frac{dL}{dq} = ln(q) - ln(p^{0}) + 1 + F' \\lambda_1 + H'\\lambda_2$ <br> <br>\n",
    " Solving for q: <br>\n",
    " <br>\n",
    " $q(\\lambda_1,\\lambda_2) = e^{ln(p^{0}) - 1 -F'\\lambda_1 - H'\\lambda_2}$\n",
    "<br> <br>\n",
    "The solution is always positive, so we do not need the $\\bigg(q>0\\bigg)$ constraint <br> <br>\n",
    "The Lagrange dual function is defined as: <br>\n",
    "<br>\n",
    "$G(\\lambda_1,\\lambda_2) = L(q(\\lambda_1,\\lambda_2),\\lambda_1,\\lambda_2)$ <br> <br>\n",
    "The optimal Lagrange multipliers follow from the maximization of the Lagrange dual function (or the minimization of the negative Lagrange dual function): <br>\n",
    "<br>\n",
    "$ (\\lambda_1^{*},\\lambda_2^{*})$ = $ argmin \\ \\bigg \\{ -G(\\lambda_1,\\lambda_2) \\bigg \\}$ <br>\n",
    "$subject \\ to$ <br>\n",
    "$\\lambda_1\\leq 0$ <br>\n",
    "<br>\n",
    "Then with the optimal lagrange multiplayers we can define the optimal set of probabilities as: <br>\n",
    "$p$ = $q(\\lambda_1^{*},\\lambda_2^{*})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41ea4f9",
   "metadata": {},
   "source": [
    "## Defining F and H  matrix for inequality and equality constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ae087",
   "metadata": {},
   "source": [
    "H is an object used to put equality constraints.\n",
    "In this case we want that the sum of our probability is equal to one\n",
    "* $Hq=h$\n",
    "* $H = [1,1...,1]$\n",
    "* $q=[p_{1},p_{2}...p_{T}]'$\n",
    "* h=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996b9ac",
   "metadata": {},
   "source": [
    "$-F_{v_1x}q > -v_{*_{1}}$ = $F_{v_1x} < v_{*_{1}}$ <br>\n",
    "$F_{v_2x}q>v_{*_{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316bb6c",
   "metadata": {},
   "source": [
    "\n",
    "* $F= \\bigg[\\begin{matrix}\n",
    "-v_{1}(x_{t})& ... & -v_{1}(x_{T}) \\\\\n",
    "v_{2}(x_{t})& ... & v_{2}(x_{T})\\end{matrix}\\bigg]$ <br>\n",
    "* $f=\\bigg[\\begin{matrix} -v_{*_{1}} \\\\ v_{*_{2}}\\end{matrix}\\bigg]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff01e68",
   "metadata": {},
   "source": [
    "## Rolling mean and standard deviation for the asset classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16e28d",
   "metadata": {},
   "source": [
    "Insert the asset classes for which you want to insert views (in this case I am gonna select all of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c401e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_distribution=epsilon.rolling(window=wndw).mean().dropna()\n",
    "vol_distribution=epsilon.rolling(window=wndw).std().dropna()\n",
    "asset_mean_distribution=x.rolling(window=wndw).mean().dropna()\n",
    "asset_vol_distribution=x.rolling(window=wndw).std().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcc9e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.Series(index=epsilon.columns)\n",
    "loc_t=pd.Series(index=epsilon.columns)\n",
    "scale_t=pd.Series(index=epsilon.columns)\n",
    "shape=pd.Series(index=epsilon.columns)\n",
    "loc_ln=pd.Series(index=epsilon.columns)\n",
    "scale_ln=pd.Series(index=epsilon.columns)\n",
    "quantile_mean=pd.Series(index=epsilon.columns)\n",
    "quantile_vol=pd.Series(index=epsilon.columns)\n",
    "v_star1=[]\n",
    "v_star2=[]\n",
    "\"Type your views on the mean, Suppose we have only views on SP500\"\n",
    "mean_views=pd.DataFrame([-0.15,-0.07,-0.02,-0.08,-0.25],index=x.columns)\n",
    "#mean_views=pd.DataFrame([-0.1],index=['SP500'])\n",
    "\"For the other asset classes we don't have views, then we are gonna stay consistent with the prior\"\n",
    "not_mean_views=pd.Series([p_0.dot(x_restricted[i]) for i in x.columns if i not in mean_views.index],index=[i for i in x.columns if i not in mean_views.index])\n",
    "'Merging the data'\n",
    "absolute_views_mean=pd.concat([mean_views,not_mean_views])\n",
    "'Type your views on volatility'\n",
    "vol_views=pd.DataFrame(list(asset_vol_distribution.max()*np.sqrt(252)),index=x.columns)\n",
    "not_vol_views=pd.Series([p_0.dot(asset_vol_distribution[i]) for i in x.columns if i not in vol_views.index],index=[i for i in x.columns if i not in vol_views.index])\n",
    "absolute_views_vol=x_restricted.T.dot(p_0).values.tolist()\n",
    "absolute_views_vol=pd.concat([vol_views,not_vol_views])\n",
    "obj_mean=absolute_views_mean[0]\n",
    "obj_vol=absolute_views_vol[0]\n",
    "for j in x.columns:\n",
    "    df.loc[j],loc_t.loc[j],scale_t.loc[j]=t.fit(asset_mean_distribution[j])\n",
    "    quantile_mean.loc[j]=t.cdf(obj_mean.loc[j]/252,df.loc[j],loc_t.loc[j],scale_t.loc[j])\n",
    "    v_star1.append(mean_distribution[j].quantile(quantile_mean.loc[j]))\n",
    "    shape.loc[j],loc_ln.loc[j],scale_ln.loc[j]=lognorm.fit(asset_vol_distribution[j])\n",
    "    quantile_vol.loc[j]=lognorm.cdf(obj_vol.loc[j]/np.sqrt(252),shape.loc[j],loc_ln.loc[j],scale_ln.loc[j])\n",
    "    v_star2.append(vol_distribution[j].quantile(quantile_vol.loc[j]))\n",
    "v_star1=pd.Series(v_star1)\n",
    "v_star2=pd.Series(v_star2)\n",
    "quantile_mean=quantile_mean.dropna()\n",
    "quantile_vol=quantile_vol.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e00c6531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SP500           -0.097301\n",
       "DOW_Jones       -0.063430\n",
       "Fixed_Income    -0.090965\n",
       "Russell3000     -0.066114\n",
       "Credit_Suisse   -0.023993\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_star1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a5a9f",
   "metadata": {},
   "source": [
    "# Get quantiles for the mean and the volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "864def32",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_vola_view_columns=[i+'_std' for i in x.columns]\n",
    "aux=list(vol_distribution[x.columns].values)\n",
    "v_star1.index=x.columns\n",
    "v_star2.index=absolute_vola_view_columns\n",
    "v_2x=pd.DataFrame(aux,index=p_0.index,columns=absolute_vola_view_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b00b34",
   "metadata": {},
   "source": [
    "I am creating two objective functions:\n",
    "* One for the case in which we have only equality constraint\n",
    "* One for the case in which we have both\n",
    "* I am not considering the case of only inequality constraint, because the constraint on the sum of probabilities=1 must be always satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9de2ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_Dual_func_eq_constr(Lmbda_vector,P_0,H_matrix,h):\n",
    "   '''Lmbda_vector is a ndarray with (k_ineq + k_eq) number of elements\n",
    "   P_0 is a series of prior probabilities with T number of elements\n",
    "   H matrix must be a dataframe K_eq(number of equality constraints) rows and T columns (T number of scenarios)\n",
    "   h is a series with equality constraints values\n",
    "   lmbda vector is an array with initial values for Lagrange multipliers\n",
    "   The function returns the objective function value to optimize '''\n",
    "   K_eq=len(h)\n",
    "   lmbda_2=Lmbda_vector[0:K_eq]\n",
    "   Lmbda_vector[K_eq:]=0\n",
    "   q=np.exp(np.log(P_0) - 1 - H_matrix.T.dot(lmbda_2))\n",
    "   Dual_func= q.T.dot(np.log(q) - np.log(P_0)) + lmbda_2.T.dot(H_matrix.dot(q)-h)\n",
    "   return - Dual_func\n",
    "\n",
    "def neg_Dual_func_constr(Lmbda_vector,P_0,F_matrix,H_matrix,f,h):\n",
    "   '''Lmbda_vector is a ndarray with (k_ineq + k_eq) number of elements\n",
    "   P_0 is a series of prior probabilities with T number of elements\n",
    "   F matrix must be a dataframe with K_ineq(number of inequality constraints) rows and T columns (T number of scenarios)\n",
    "   H matrix must be a dataframe K_eq(number of equality constraints) rows and T columns (T number of scenarios)\n",
    "   f is a series with intensity views for inequality constraints \n",
    "   h is a sereis with intensity views for equality constraints\n",
    "   lmbda vector is an array with initial values for Lagrange multipliers\n",
    "   The function returns the objective function value to optimize'''\n",
    "\n",
    "   K_eq=len(h)\n",
    "   K_ineq=len(f)\n",
    "   lmbda_1=Lmbda_vector[K_eq:K_ineq+1]\n",
    "   lmbda_2=Lmbda_vector[0:K_eq]\n",
    "   q=np.exp(np.log(P_0) - 1 - F_matrix.T.dot(lmbda_1) - H_matrix.T.dot(lmbda_2))\n",
    "   Dual_func=  q.T.dot(np.log(q) - np.log(P_0)) + lmbda_1.T.dot(F_matrix.dot(q)-f) + lmbda_2.T.dot(H_matrix.dot(q)-h)\n",
    "   return - Dual_func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6d740",
   "metadata": {},
   "source": [
    "## KKT Conditions \n",
    "* $\\lambda_1(Fq - f)=0$\n",
    "* $Hq=h$\n",
    "* $ Fq - f \\geq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ba568b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda1_fun_eq(x,F_matrix,H_matrix,f,h):\n",
    "     K_ineq=len(f)\n",
    "     K_eq=len(h)\n",
    "     lmbda_1=x[K_eq:K_ineq+1] # Lagrange multipliers for inequality constraints\n",
    "     lmbda_2=x[0:K_eq]        # Lagrange multipliers for equality constraints\n",
    "     q=np.exp(np.log(p_0) - 1 - F_matrix.T.dot(lmbda_1) - H_matrix.T.dot(lmbda_2))\n",
    "     return lmbda_1*(F_matrix.dot(q)-f)\n",
    "\n",
    "def lambda2_fun_eq(x,F_matrix,H_matrix,f,h,Obj_fun):\n",
    "    function=Obj_fun\n",
    "    if function == neg_Dual_func_constr:\n",
    "        K_ineq=len(f)\n",
    "        K_eq=len(h)\n",
    "        lmbda_1=x[K_eq:K_ineq+1] # Lagrange multipliers for inequality constraints\n",
    "        lmbda_2=x[0:K_eq]        # Lagrange multipliers for equality constraints \n",
    "        q=np.exp(np.log(p_0) - 1 - F_matrix.T.dot(lmbda_1) - H_matrix.T.dot(lmbda_2))\n",
    "    else:\n",
    "         K_eq=len(h)\n",
    "         lmbda_2=x[0:K_eq]\n",
    "         q=np.exp(np.log(p_0) - 1 - H_matrix.T.dot(lmbda_2))\n",
    "    return H_matrix.dot(q)- h\n",
    "\n",
    "def ineq_cons(x,F_matrix,H_matrix,f,h):\n",
    "    K_ineq=len(f)\n",
    "    K_eq=len(h)\n",
    "    lmbda_1=x[K_eq:K_ineq+1] # Lagrange multipliers for inequality constraints\n",
    "    lmbda_2=x[0:K_eq]        # Lagrange multipliers for equality constraints \n",
    "    q=np.exp(np.log(p_0) - 1 - F_matrix.T.dot(lmbda_1) - H_matrix.T.dot(lmbda_2))\n",
    "    return F_matrix.dot(q)-f\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a7b006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#absolute_views_index=absolute_views_mean_columns+ absolute_vola_view_columns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30141cea",
   "metadata": {},
   "source": [
    "We have to determine the sign of the view, the optimizer works with > constraints\n",
    "* in case of positive views we leave the original sign\n",
    "* in case of negative views we have to change the sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a5efc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_view_mean_sign=['-','-','-','-','-']\n",
    "absolute_view_vol_sign=['+','+','+','+','+']\n",
    "v_1x_aux=v_1x.copy()\n",
    "v_2x_aux=v_2x.copy()\n",
    "v_star1_aux=v_star1.copy() \n",
    "v_star2_aux=v_star2.copy() \n",
    "for i in np.arange(0,len(absolute_view_mean_sign)):\n",
    "    if absolute_view_mean_sign[i]=='+':\n",
    "        continue\n",
    "    else: \n",
    "        v_1x_aux.iloc[i]=-v_1x.iloc[i]\n",
    "        v_star1_aux.iloc[i]=-v_star1.iloc[i]\n",
    "\n",
    "for i in np.arange(0,len(absolute_view_vol_sign)):\n",
    "    if absolute_view_mean_sign[i]=='+':\n",
    "        continue\n",
    "    else: \n",
    "        v_2x_aux.iloc[i]=-v_2x.iloc[i]\n",
    "        v_star2_aux.iloc[i]=-v_star2.iloc[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8789da66",
   "metadata": {},
   "source": [
    "Now we need to alternate views on mean and volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e027a36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SP500           -0.097301\n",
       "DOW_Jones       -0.063430\n",
       "Fixed_Income    -0.090965\n",
       "Russell3000     -0.066114\n",
       "Credit_Suisse   -0.023993\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_star1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fdc004c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.004439072466965118\n",
      "            Iterations: 11\n",
      "            Function evaluations: 45\n",
      "            Gradient evaluations: 11\n",
      "     fun: -0.004439072466965118\n",
      "     jac: array([-4.32133675e-06, -9.63138300e-05, -3.10461341e+00])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 45\n",
      "     nit: 11\n",
      "    njev: 11\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.99516186, -0.09534603,  0.        ])\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.001780419594354037\n",
      "            Iterations: 12\n",
      "            Function evaluations: 49\n",
      "            Gradient evaluations: 12\n",
      "     fun: -0.001780419594354037\n",
      "     jac: array([-2.09810387e-05, -7.14410955e-04, -3.00386927e+00])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 49\n",
      "     nit: 12\n",
      "    njev: 12\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.99798033, -0.05991419,  0.        ])\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.006964498051625538\n",
      "            Iterations: 8\n",
      "            Function evaluations: 33\n",
      "            Gradient evaluations: 8\n",
      "     fun: -0.006964498051625538\n",
      "     jac: array([-4.46045306e-07, -7.76490197e-06, -2.31364148e+00])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 33\n",
      "     nit: 8\n",
      "    njev: 8\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-9.96213674e-01, -1.18186351e-01, -1.05879118e-22])\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.0019699350614790505\n",
      "            Iterations: 11\n",
      "            Function evaluations: 45\n",
      "            Gradient evaluations: 11\n",
      "     fun: -0.0019699350614790505\n",
      "     jac: array([-1.81811629e-06, -5.74548903e-05, -3.04339229e+00])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 45\n",
      "     nit: 11\n",
      "    njev: 11\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.99775802, -0.0637066 ,  0.        ])\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.0003336851974512166\n",
      "            Iterations: 14\n",
      "            Function evaluations: 57\n",
      "            Gradient evaluations: 14\n",
      "     fun: -0.0003336851974512166\n",
      "     jac: array([-4.91745595e-06, -3.74074505e-04, -3.09785347e+00])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 57\n",
      "     nit: 14\n",
      "    njev: 14\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.99970482, -0.02621316,  0.        ])\n"
     ]
    }
   ],
   "source": [
    "post_prob=pd.DataFrame(index=p_0.index)\n",
    "for i in x.columns:\n",
    "    H=pd.DataFrame(np.ones(len(p_0)),index=p_0.index,columns=['ones'])\n",
    "    h=pd.Series([1],index=H.columns)\n",
    "    F=pd.DataFrame(index=p_0.index)\n",
    "    f=pd.Series()\n",
    "    if i not in mean_views.index:\n",
    "        H=pd.concat([H,v_1x[i]],axis=1)\n",
    "        h=pd.concat[(h,pd.Series(v_star1.loc[i]))]\n",
    "    else:\n",
    "        F=pd.concat([F,v_1x_aux[i]],axis=1)\n",
    "        f=pd.concat([f,pd.Series(v_star1_aux.loc[i])])\n",
    "    if i not in vol_views.index:\n",
    "        H=pd.concat([H,v_2x[i+'_std']],axis=1)\n",
    "        h=pd.concat[(h,pd.series(v_star2.loc[i+'_std']))]\n",
    "    else:\n",
    "        F=pd.concat([F,v_2x[i+'_std']],axis=1)\n",
    "        f=pd.concat([f,pd.Series(v_star2_aux.loc[i+'_std'])])\n",
    "    K_eq=len(h)\n",
    "    K_ineq=len(f)\n",
    "    # initial guess\n",
    "    lmbda_vector_0=np.ones(K_eq+K_ineq)\n",
    "    lmbda_vector_0[K_eq:K_ineq+1]=-1\n",
    "    lmbda_2=lmbda_vector_0[0:K_eq]       # Lagrange multipliers for equality constraints\n",
    "    lmbda_1=lmbda_vector_0[K_eq:K_ineq+1]# Lagrange multipliers for inequality constraints\n",
    "    F=F.T\n",
    "    H=H.T\n",
    "    f.index=F.index\n",
    "    if (K_eq!=0) & (K_ineq!=0):\n",
    "        obj_fun= neg_Dual_func_constr\n",
    "    else:\n",
    "        obj_fun= neg_Dual_func_eq_constr\n",
    "    if K_ineq!=0:\n",
    "        obj_fun(lmbda_vector_0,time_cond_prob,F,H,f,h) # value of the negative dual function\n",
    "    else:\n",
    "        obj_fun(lmbda_vector_0,time_cond_prob,H,h) #value of the negative dual function with only equality constraints\n",
    "    if (K_ineq!=0):\n",
    "        cons =    ({'type': 'eq', 'fun': lambda1_fun_eq, 'args': (F,H,f,h)},\n",
    "           {'type': 'eq', 'fun': lambda2_fun_eq, 'args': (F,H,f,h,obj_fun)},\n",
    "           {'type': 'ineq', 'fun': ineq_cons,    'args': (F,H,f,h)})\n",
    "        arguments=(p_0,F,H,f,h)\n",
    "    else: \n",
    "        cons = ({'type': 'eq', 'fun': lambda2_fun_eq, 'args': (F,H,f,h,obj_fun)})\n",
    "        arguments=(p_0,H,h)\n",
    "    if (K_ineq!=0):\n",
    "        bnds= [(None, 0) for _ in range(K_ineq)]\n",
    "        bnds=[(None,None)]+bnds\n",
    "    else:\n",
    "        bnds= [(None,None) for _ in range(K_eq)]\n",
    "    res=spopt.minimize(obj_fun,lmbda_vector_0,method='SLSQP',args=arguments,bounds=bnds,constraints=cons,options={'maxiter':200,'disp': True})\n",
    "    Lagrangian_mltps=res.x\n",
    "    print(res)\n",
    "    lmbda_2=Lagrangian_mltps[0:K_eq]\n",
    "    lmbda_1=Lagrangian_mltps[K_eq:K_ineq+1]\n",
    "    post_prob[i]=np.exp(np.log(p_0) - 1 - F.T.dot(lmbda_1) - H.T.dot(lmbda_2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29ff70d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SP500            0.041522\n",
       "DOW_Jones        0.037379\n",
       "Fixed_Income     0.000426\n",
       "Russell3000      0.041162\n",
       "Credit_Suisse    0.152261\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(post_prob*x_restricted**2).sum()*252"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e95d73",
   "metadata": {},
   "source": [
    "$ Fq - f \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8c074",
   "metadata": {},
   "source": [
    "## Sample Expected values and variances\n",
    " * $E(X)=\\frac{1}{N}  \\sum_{t=1}^{T} X_{t}$\n",
    " * $Var(X)= E(X^2) -E(X)^2 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae734e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP500            0.086135\n",
      "DOW_Jones        0.081408\n",
      "Fixed_Income     0.010184\n",
      "Russell3000      0.086689\n",
      "Credit_Suisse   -0.112496\n",
      "dtype: float64\n",
      "SP500            0.205083\n",
      "DOW_Jones        0.194883\n",
      "Fixed_Income     0.021027\n",
      "Russell3000      0.207063\n",
      "Credit_Suisse    0.403497\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(x_restricted.mean()*252)\n",
    "print(x_restricted.std()*np.sqrt(252))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae9ece",
   "metadata": {},
   "source": [
    "## Posterior Expected values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d961a6",
   "metadata": {},
   "source": [
    "$E^{v}(R)=\\sum_{t=1}^{T} p_{t} \\cdot X_{t}$ <br>\n",
    "     NX1 vector of asset classes'posterior returns\n",
    "<br>\n",
    "<br>\n",
    "$Var^{v}(X)= E(X^2) - E(X)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00a5a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  SP500  DOW_Jones  Fixed_Income  Russell3000  Credit_Suisse\n",
      "SP500          0.091146   0.091136      0.078470     0.091011       0.094839\n",
      "DOW_Jones      0.086978   0.086983      0.074067     0.086856       0.090601\n",
      "Fixed_Income   0.003087   0.003086      0.011873     0.003088       0.002914\n",
      "Russell3000    0.090347   0.090344      0.077050     0.090215       0.094129\n",
      "Credit_Suisse -0.141890  -0.141889     -0.175922    -0.142009      -0.125550\n",
      "                  SP500  DOW_Jones  Fixed_Income  Russell3000  Credit_Suisse\n",
      "SP500          0.199555   0.199555      0.199587     0.199556       0.199490\n",
      "DOW_Jones      0.191344   0.191344      0.191289     0.191345       0.191285\n",
      "Fixed_Income   0.020507   0.020507      0.020457     0.020508       0.020503\n",
      "Russell3000    0.202236   0.202236      0.202282     0.202238       0.202164\n",
      "Credit_Suisse  0.389110   0.389110      0.389063     0.389110       0.389160\n"
     ]
    }
   ],
   "source": [
    "E_X=x_restricted.T.dot(post_prob)\n",
    "sec_mom=(x_restricted**2).T.dot(post_prob)\n",
    "vol_X=np.sqrt(sec_mom-E_X**2)\n",
    "print(E_X*252)\n",
    "print(vol_X*np.sqrt(252))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ada47",
   "metadata": {},
   "source": [
    "## Sample covariance = Covariance with equally weighted probabilities\n",
    "Cov(X,Y)= $\\frac{1}{N}  \\sum_{t=1}^{T} (X_{t}-E(X))(Y_{t}-E(Y))$\n",
    "* with $\\frac{1}{N}=$ equally weighted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38956ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               SP500  DOW_Jones  Fixed_Income  Russell3000  Credit_Suisse\n",
      "SP500           1.00       0.95         -0.19         0.96           0.46\n",
      "DOW_Jones       0.95       1.00         -0.19         0.97           0.47\n",
      "Fixed_Income   -0.19      -0.19          1.00        -0.19          -0.21\n",
      "Russell3000     0.96       0.97         -0.19         1.00           0.46\n",
      "Credit_Suisse   0.46       0.47         -0.21         0.46           1.00\n"
     ]
    }
   ],
   "source": [
    "sample_corr=round(x_restricted.corr(),2)\n",
    "print(sample_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d82a12",
   "metadata": {},
   "source": [
    "## Posterior Covariance  \n",
    "$Cov^{post}(X,Y)$= $\\sum_{t=1}^{T} p_{t} \\cdot(X_{t}-E(X))(Y_{t}-E(Y))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d5169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SP500</th>\n",
       "      <th>DOW_Jones</th>\n",
       "      <th>Fixed_Income</th>\n",
       "      <th>Russell3000</th>\n",
       "      <th>Credit_Suisse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP500</th>\n",
       "      <td>SP500            0.000158\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000152\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000016\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.00016\n",
       "DOW_Jones        0.00...</td>\n",
       "      <td>SP500            0.000308\n",
       "DOW_Jones        0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOW_Jones</th>\n",
       "      <td>SP500            0.000152\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000145\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000016\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000154\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000295\n",
       "DOW_Jones        0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fixed_Income</th>\n",
       "      <td>SP500            0.000016\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000016\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000002\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000016\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000032\n",
       "DOW_Jones        0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russell3000</th>\n",
       "      <td>SP500            0.00016\n",
       "DOW_Jones        0.00...</td>\n",
       "      <td>SP500            0.000154\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000016\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000162\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000312\n",
       "DOW_Jones        0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_Suisse</th>\n",
       "      <td>SP500            0.000308\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000295\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000032\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000312\n",
       "DOW_Jones        0.0...</td>\n",
       "      <td>SP500            0.000601\n",
       "DOW_Jones        0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           SP500  \\\n",
       "SP500          SP500            0.000158\n",
       "DOW_Jones        0.0...   \n",
       "DOW_Jones      SP500            0.000152\n",
       "DOW_Jones        0.0...   \n",
       "Fixed_Income   SP500            0.000016\n",
       "DOW_Jones        0.0...   \n",
       "Russell3000    SP500            0.00016\n",
       "DOW_Jones        0.00...   \n",
       "Credit_Suisse  SP500            0.000308\n",
       "DOW_Jones        0.0...   \n",
       "\n",
       "                                                       DOW_Jones  \\\n",
       "SP500          SP500            0.000152\n",
       "DOW_Jones        0.0...   \n",
       "DOW_Jones      SP500            0.000145\n",
       "DOW_Jones        0.0...   \n",
       "Fixed_Income   SP500            0.000016\n",
       "DOW_Jones        0.0...   \n",
       "Russell3000    SP500            0.000154\n",
       "DOW_Jones        0.0...   \n",
       "Credit_Suisse  SP500            0.000295\n",
       "DOW_Jones        0.0...   \n",
       "\n",
       "                                                    Fixed_Income  \\\n",
       "SP500          SP500            0.000016\n",
       "DOW_Jones        0.0...   \n",
       "DOW_Jones      SP500            0.000016\n",
       "DOW_Jones        0.0...   \n",
       "Fixed_Income   SP500            0.000002\n",
       "DOW_Jones        0.0...   \n",
       "Russell3000    SP500            0.000016\n",
       "DOW_Jones        0.0...   \n",
       "Credit_Suisse  SP500            0.000032\n",
       "DOW_Jones        0.0...   \n",
       "\n",
       "                                                     Russell3000  \\\n",
       "SP500          SP500            0.00016\n",
       "DOW_Jones        0.00...   \n",
       "DOW_Jones      SP500            0.000154\n",
       "DOW_Jones        0.0...   \n",
       "Fixed_Income   SP500            0.000016\n",
       "DOW_Jones        0.0...   \n",
       "Russell3000    SP500            0.000162\n",
       "DOW_Jones        0.0...   \n",
       "Credit_Suisse  SP500            0.000312\n",
       "DOW_Jones        0.0...   \n",
       "\n",
       "                                                   Credit_Suisse  \n",
       "SP500          SP500            0.000308\n",
       "DOW_Jones        0.0...  \n",
       "DOW_Jones      SP500            0.000295\n",
       "DOW_Jones        0.0...  \n",
       "Fixed_Income   SP500            0.000032\n",
       "DOW_Jones        0.0...  \n",
       "Russell3000    SP500            0.000312\n",
       "DOW_Jones        0.0...  \n",
       "Credit_Suisse  SP500            0.000601\n",
       "DOW_Jones        0.0...  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.DataFrame(index=x_restricted.columns,columns=x_restricted.columns)\n",
    "for i in np.arange(0,len(test)):\n",
    "    for j in np.arange(0,len(test)):\n",
    "        test.iloc[i,j]=vol_X.iloc[i]*vol_X.iloc[j]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760e9dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot handle multidimensional aweights",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucadaquanno/Desktop/Documents/CIOS.Analyse/Return_forecasting/tests/Conditional_bootstrapping/Entropy_pool_prob_300323.ipynb Cell 73\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucadaquanno/Desktop/Documents/CIOS.Analyse/Return_forecasting/tests/Conditional_bootstrapping/Entropy_pool_prob_300323.ipynb#Z1436sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Cov_post\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame(np\u001b[39m.\u001b[39;49mcov(x_restricted\u001b[39m.\u001b[39;49mT,aweights\u001b[39m=\u001b[39;49mpost_prob),index\u001b[39m=\u001b[39mx_restricted\u001b[39m.\u001b[39mcolumns,columns\u001b[39m=\u001b[39mx_restricted\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucadaquanno/Desktop/Documents/CIOS.Analyse/Return_forecasting/tests/Conditional_bootstrapping/Entropy_pool_prob_300323.ipynb#Z1436sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vol_coeff\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame(np\u001b[39m.\u001b[39mkron(vol_X,vol_X)\u001b[39m.\u001b[39mreshape((\u001b[39m5\u001b[39m,\u001b[39m5\u001b[39m)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucadaquanno/Desktop/Documents/CIOS.Analyse/Return_forecasting/tests/Conditional_bootstrapping/Entropy_pool_prob_300323.ipynb#Z1436sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vol_coeff\u001b[39m.\u001b[39mindex\u001b[39m=\u001b[39mCov_post\u001b[39m.\u001b[39mindex\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mcov\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2505\u001b[0m, in \u001b[0;36mcov\u001b[0;34m(m, y, rowvar, bias, ddof, fweights, aweights, dtype)\u001b[0m\n\u001b[1;32m   2503\u001b[0m aweights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(aweights, dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n\u001b[1;32m   2504\u001b[0m \u001b[39mif\u001b[39;00m aweights\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2505\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2506\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcannot handle multidimensional aweights\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2507\u001b[0m \u001b[39mif\u001b[39;00m aweights\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[1;32m   2508\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2509\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mincompatible numbers of samples and aweights\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot handle multidimensional aweights"
     ]
    }
   ],
   "source": [
    "Cov_post=pd.DataFrame(np.cov(x_restricted.T,aweights=post_prob),index=x_restricted.columns,columns=x_restricted.columns)\n",
    "vol_coeff=pd.DataFrame(np.kron(vol_X,vol_X).reshape((5,5)))\n",
    "vol_coeff.index=Cov_post.index\n",
    "vol_coeff.columns=Cov_post.columns\n",
    "print(vol_coeff)\n",
    "print(Cov_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf3173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               SP500  DOW_Jones  Fixed_Income  Russell3000  Credit_Suisse\n",
      "SP500           1.00       0.96         -0.14         0.98           0.49\n",
      "DOW_Jones       0.96       1.00         -0.14         0.97           0.49\n",
      "Fixed_Income   -0.14      -0.14          1.00        -0.14          -0.18\n",
      "Russell3000     0.98       0.97         -0.14         1.00           0.49\n",
      "Credit_Suisse   0.49       0.49         -0.18         0.49           1.00\n",
      "               SP500  DOW_Jones  Fixed_Income  Russell3000  Credit_Suisse\n",
      "SP500           1.00       0.95         -0.19         0.96           0.46\n",
      "DOW_Jones       0.95       1.00         -0.19         0.97           0.47\n",
      "Fixed_Income   -0.19      -0.19          1.00        -0.19          -0.21\n",
      "Russell3000     0.96       0.97         -0.19         1.00           0.46\n",
      "Credit_Suisse   0.46       0.47         -0.21         0.46           1.00\n"
     ]
    }
   ],
   "source": [
    "#print(vol_coeff)\n",
    "#print(Cov_post)\n",
    "corr_post=round(Cov_post.divide(vol_coeff),2)\n",
    "print(corr_post)\n",
    "print(sample_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b6025",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e6bd80",
   "metadata": {},
   "source": [
    "Mixing Probabilities, Priors and Kernels via Entropy Pooling https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1944303 <br>\n",
    "Fully Flexible Views: Theory and Practice https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1213325"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0318f38efd34f66a233dc4c6df2ab6f566e47d01e77740aef179e188cc41a779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
