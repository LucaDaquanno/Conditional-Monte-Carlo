{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c859ce61",
   "metadata": {},
   "source": [
    "# Flexible probabilities for scenario analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd45759",
   "metadata": {},
   "source": [
    "__Importing libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355ed263",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucadaquanno/Desktop/Documents/CIOS.Analyse/Return_forecasting/tests/Conditional_bootstrapping/Entropy_pool_prob_300323.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucadaquanno/Desktop/Documents/CIOS.Analyse/Return_forecasting/tests/Conditional_bootstrapping/Entropy_pool_prob_300323.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moperator\u001b[39;00m \u001b[39mimport\u001b[39;00m itemgetter\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucadaquanno/Desktop/Documents/CIOS.Analyse/Return_forecasting/tests/Conditional_bootstrapping/Entropy_pool_prob_300323.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstatsmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgraphics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtsaplots\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_acf, plot_pacf\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lucadaquanno/Desktop/Documents/CIOS.Analyse/Return_forecasting/tests/Conditional_bootstrapping/Entropy_pool_prob_300323.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39march\u001b[39;00m \u001b[39mimport\u001b[39;00m arch_model\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucadaquanno/Desktop/Documents/CIOS.Analyse/Return_forecasting/tests/Conditional_bootstrapping/Entropy_pool_prob_300323.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m gaussian_kde\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucadaquanno/Desktop/Documents/CIOS.Analyse/Return_forecasting/tests/Conditional_bootstrapping/Entropy_pool_prob_300323.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/arch/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_version\u001b[39;00m \u001b[39mimport\u001b[39;00m version \u001b[39mas\u001b[39;00m __version__, version_tuple\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39munivariate\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmean\u001b[39;00m \u001b[39mimport\u001b[39;00m arch_model\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutility\u001b[39;00m \u001b[39mimport\u001b[39;00m test\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdoc\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/arch/univariate/__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m annotations\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39march\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39munivariate\u001b[39;00m \u001b[39mimport\u001b[39;00m recursions_python\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39march\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39munivariate\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribution\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Distribution,\n\u001b[1;32m      8\u001b[0m     GeneralizedError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     StudentsT,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39march\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39munivariate\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmean\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     ARX,\n\u001b[1;32m     15\u001b[0m     HARX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     arch_model,\n\u001b[1;32m     21\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/arch/univariate/recursions_python.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mPure Python implementations of the core recursions in the models. Only used for\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mtesting and if it is not possible to install the Cython version using\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mexport ARCH_NO_BINARY=1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mpython -m pip install .\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m annotations\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39march\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumba\u001b[39;00m \u001b[39mimport\u001b[39;00m jit\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabc\u001b[39;00m \u001b[39mimport\u001b[39;00m ABCMeta, abstractmethod\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional, Union, cast\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/arch/compat/numba.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m DISABLE_NUMBA:\n\u001b[1;32m     19\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m \u001b[39mimport\u001b[39;00m jit\n\u001b[1;32m     23\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(x: \u001b[39mfloat\u001b[39m, y: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numba/__init__.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdel\u001b[39;00m get_versions\n\u001b[1;32m     16\u001b[0m \u001b[39mdel\u001b[39;00m generate_version_info\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m types, errors\n\u001b[1;32m     22\u001b[0m \u001b[39m# Re-export typeof\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numba/core/config.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     _HAVE_YAML \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mllvmlite\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbinding\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mll\u001b[39;00m\n\u001b[1;32m     17\u001b[0m IS_WIN32 \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mplatform\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mwin32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m IS_OSX \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mplatform\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mdarwin\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/llvmlite/binding/__init__.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mThings that rely on the LLVM library\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdylib\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexecutionengine\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minitfini\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/llvmlite/binding/dylib.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mctypes\u001b[39;00m \u001b[39mimport\u001b[39;00m c_void_p, c_char_p, c_bool, POINTER\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllvmlite\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbinding\u001b[39;00m \u001b[39mimport\u001b[39;00m ffi\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllvmlite\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbinding\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m _encode_string\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maddress_of_symbol\u001b[39m(name):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/llvmlite/binding/ffi.py:186\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m _lib_path \u001b[39min\u001b[39;00m _lib_paths:\n\u001b[1;32m    185\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m         lib \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39;49mCDLL(_lib_path)\n\u001b[1;32m    187\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    188\u001b[0m         errors\u001b[39m.\u001b[39mappend(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/ctypes/__init__.py:382\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FuncPtr \u001b[39m=\u001b[39m _FuncPtr\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[1;32m    383\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m handle\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import scipy.stats as sts\n",
    "from scipy.stats import norm,chi2,t,lognorm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import statistics\n",
    "import time\n",
    "import plotly as plty\n",
    "import scipy.optimize as spopt\n",
    "import datetime\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from arch import arch_model\n",
    "from scipy.stats import gaussian_kde\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=1\n",
    "if user ==1:\n",
    "    path = \"/Users/lucadaquanno/Desktop/Documents/CIOS.Analyse/Return_forecasting/Entropy_pooling_python/\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9d0b8",
   "metadata": {},
   "source": [
    "## Connecting the API and send Time series requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_flex(list_of_ISIN, start_date, end_date, **kwargs):\n",
    "    list_of_dcts=[]\n",
    "    for e in list_of_ISIN:\n",
    "        d={\"code\": e, \"code_type\": \"isin\"}\n",
    "        list_of_dcts.append(d)\n",
    "    dct_body={\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"instruments\": list_of_dcts,\n",
    "        \"convert_prices\": False,\n",
    "        \"use_live_data\": True,\n",
    "        \"extend_timeseries_in_db\": False,\n",
    "        \"extend_investment_universe\": False,\n",
    "        \"source\": \"merged\"\n",
    "    }\n",
    "    dct_body.update(kwargs)\n",
    "    body = json.dumps(dct_body)\n",
    "    r = requests.post(\"https://data.acp-cios.fincite.net/api/v1/timeseries/\", data=body,\n",
    "                         headers = {\n",
    "                             'content-type':'application/json',\n",
    "                             'authorization':'Bearer L0hxZj2udrAgY1QxqW1rG5HkshYR0EY8AU9QMtDM'})\n",
    "    return json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af43b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "isin=[\"US78378X1072\",\"US2605661048\",\"IE0031719473\",\"US4642876894\",\"CH0012138530\"]\n",
    "start_date='2000-12-31'\n",
    "end_date='2022-12-31'\n",
    "response=time_series_flex(isin, start_date, end_date)\n",
    "response_list=response['response']['instruments']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09174b3",
   "metadata": {},
   "source": [
    "## Transforming the Response into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4fb558",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "for k in response_list:\n",
    "    response_dict=k['timeseries']\n",
    "    dates_index = list(map(itemgetter('date'), response_dict))\n",
    "    dates_index=[datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates_index]\n",
    "    close_prices=list(map(itemgetter('close_price'), response_dict))\n",
    "    prices=pd.DataFrame(close_prices,dates_index)\n",
    "    #x=np.log(prices).diff().dropna()\n",
    "    #x=x.resample('M').sum()\n",
    "    df=pd.concat([df,prices],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014d40d",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c90ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily data\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_excel(path+\"dsws_timeseries.xlsx\", parse_dates = [\"date\"], index_col=(\"date\") )\n",
    "start_date = df.index.min()\n",
    "end_date  = df.index.max() #  last available date\n",
    "df = df[start_date : end_date]\n",
    "bdate = pd.bdate_range(start_date, end_date) # get only business day\n",
    "x = df.copy()\n",
    "for i in bdate:\n",
    "    if (i in x.index) == False: #checking missing values\n",
    "        x.loc[i,:] = np.nan\n",
    "x = x.sort_index(ascending=True)\n",
    "spline = False\n",
    "if spline:\n",
    "    x = x.interpolate(method = \"cubic\")\n",
    "else:\n",
    "    x = x.fillna(method = \"ffill\")\n",
    "name = ['SP500','DOW_Jones','Fixed_Income','Russell3000','Credit_Suisse']\n",
    "x.columns = name\n",
    "dates=x.index\n",
    "x=x.pct_change().dropna()\n",
    "#x=np.log(x).diff().dropna()\n",
    "Time_scaling={'daily':'d','monthly':'m','yearly':'y'}\n",
    "data_frequency='daily'\n",
    "scaling_factor=Time_scaling[data_frequency]\n",
    "if scaling_factor=='m':\n",
    "    x=(1+x).resample('M').prod()-1\n",
    "    print('monthly data')\n",
    "elif scaling_factor=='y':\n",
    "    x=(1+x).resample('Y').prod()-1\n",
    "    print('yearly data')\n",
    "else:\n",
    "    print('daily data')\n",
    "\n",
    "#x.index=np.arange(0,len(x))\n",
    "#print(x.loc[x.index[0]:x.index[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61ff37",
   "metadata": {},
   "source": [
    "### Defining our prior: time-conditioned probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240487d",
   "metadata": {},
   "source": [
    "Tipically we need to rely more on recent scenarios and possibly on additional information on the market. <br>\n",
    "This leads to alternative specifications of probabilities based on the notions of time conditioning and state conditioning respectively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7c380",
   "metadata": {},
   "source": [
    "In the time conditioning approach the relative weight of each scenario depends on the time elapsed. <br>\n",
    "1\\. $ p_t|\\tau_{HL}$ := $pe$ $^{-\\frac{ln(2)}{\\tau_{HL}}|t - T|}$ <br>\n",
    "2\\. $p$ := 1/ $ \\sum_{t}^{} e^{-\\frac{ln(2)}{\\tau_{HL}}|t - T|}$  <br>\n",
    "\n",
    "* $\\tau_{HL}$ can be interpreted as the  time required for the probability of a scenario to decrease to half of its maximum value in $T$  <br>\n",
    "* the lower is $\\tau_{HL}$ the higher is the decay rate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exp_Decay_prob(X,T_date,Tau_date,data_freq=scaling_factor):\n",
    "    '''X is the timeseries of risk_drivers\n",
    "    T_date is the latest observation's date\n",
    "    Tau_date is the date for the half life parameter\n",
    "    this function return a series of time-decaying probabilities'''\n",
    "    if data_freq=='d':\n",
    "        X=X.loc[:T_date]\n",
    "        Tau_integer=X.loc[:Tau_date].shape[0] # associating an integer to the Tau_date\n",
    "        T_integer=X.shape[0]                 # associating an integer to the T date\n",
    "        exponent=[-(np.log(2)/Tau_integer)*abs((t-T_integer))for t in np.arange(0,T_integer)]\n",
    "        P=1/np.sum(np.exp(exponent))\n",
    "        time_conditioned_p=P*np.exp(exponent)\n",
    "        return pd.Series(time_conditioned_p,name='T_cond_prob',index=X.index)\n",
    "    elif data_freq == 'm':\n",
    "        X=X.loc[:T_date]\n",
    "        Tau_integer=X.loc[:Tau_date].shape[0] # associating an integer to the Tau_date\n",
    "        T_integer=X.shape[0]              # associating an integer to the T date\n",
    "        X=X.loc[:T_date]\n",
    "        exponent=[-(np.log(2)/Tau_integer)*abs((t-T_integer))for t in np.arange(0,T_integer)]\n",
    "        P=1/np.sum(np.exp(exponent))\n",
    "        time_conditioned_p=pd.Series(P*np.exp(exponent),name='T_cond_prob',index=X.index)\n",
    "        return time_conditioned_p.resample('M').sum()\n",
    "    else:\n",
    "        X=X.loc[:T_date]\n",
    "        Tau_integer=X.loc[:Tau_date].shape[0] # associating an integer to the Tau_date\n",
    "        T_integer=X.shape[0]              # associating an integer to the T date\n",
    "        X=X.loc[:T_date]\n",
    "        exponent=[-(np.log(2)/Tau_integer)*abs((t-T_integer))for t in np.arange(0,T_integer)]\n",
    "        P=1/np.sum(np.exp(exponent))\n",
    "        time_conditioned_p=pd.Series(P*np.exp(exponent),name='T_cond_prob',index=X.index)\n",
    "        return time_conditioned_p.resample('Y').sum()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115d3271",
   "metadata": {},
   "source": [
    "To express our views on volatility, we may need to consider a restricted dataset (observations - rolling_window) and initialize a prior distribution based on this limited information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63199cd",
   "metadata": {},
   "source": [
    "With Re-scaled data the optimizer works better, we are going to apply a z-score normalization on our original dataset <br>\n",
    "$ \\epsilon= \\frac{x - \\bar{x}}{\\sigma(x)}$\n",
    "* $\\bar{x}$ is the returns sample mean\n",
    "* $\\sigma(x)$ is the returns standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd959c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaling_factor=='d':\n",
    "    wndw=252\n",
    "elif scaling_factor=='m':\n",
    "    wndw=12\n",
    "else:\n",
    "    wndw=2\n",
    "#x_r=x.iloc[0:(len(x)-wndw+1)]\n",
    "x_restricted=x.iloc[wndw-1:]\n",
    "data_sample_mean=x.mean()\n",
    "data_sample_volat=x.std()\n",
    "std_data=(x-data_sample_mean)/data_sample_volat\n",
    "epsilon=std_data.copy()\n",
    "epsilon_restricted=epsilon.iloc[wndw-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e55fda",
   "metadata": {},
   "source": [
    "## Testing the function for the prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431502fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tau_date='2020-01-04'\n",
    "T_date=epsilon_restricted.index[-1]\n",
    "time_cond_prob= Exp_Decay_prob(epsilon_restricted,T_date,tau_date)\n",
    "print(np.sum(time_cond_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_decay_flag=True\n",
    "if exp_decay_flag:\n",
    "    p_0=time_cond_prob\n",
    "else: #equally weighted probability as a prior\n",
    "    p_0=pd.Series(np.ones(len(epsilon_restricted))*1/len(epsilon_restricted),index=epsilon_restricted.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd964e35",
   "metadata": {},
   "source": [
    "## Defining the user views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c5d1de",
   "metadata": {},
   "source": [
    "The most simple function $v_1(X)$ we can think about, is the function that maps our risk drivers $X$ in a portfolio.\n",
    "A function mapping an N-dimensional object to a one-dimensional object.\n",
    "* $V_1(X) :=  Xw $\n",
    "* $\\mathbb{E}_p{\\bigg(V_1(X)\\bigg)} := p'V_1(X) $\n",
    "* $V:=E_{p}{V_1(X)}\\geq v_{*_{1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v_1x=epsilon_restricted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad14d45",
   "metadata": {},
   "source": [
    "# Prior Expected values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af97116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SP500            0.003180\n",
       "DOW_Jones        0.003002\n",
       "Fixed_Income    -0.026547\n",
       "Russell3000      0.002915\n",
       "Credit_Suisse   -0.002746\n",
       "dtype: float64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_1x.T.dot(p_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_frequency=='daily':\n",
    "    scaling_adjustment=252\n",
    "elif data_frequency=='monthly':\n",
    "    scaling_adjustment=22\n",
    "else:\n",
    "    scaling_adjustment=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1656e9",
   "metadata": {},
   "source": [
    "* $v_{*_{1}} \\approx -10\\%$ (yearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68104cf",
   "metadata": {},
   "source": [
    "Suppose we have  a bearish views for our portfolio, we can state our view as follows : <br>\n",
    "* $V:E_{p}{v_1(X)}\\leq v_{*_{1}}$\n",
    "* $\\bigg[p'[Xw]\\bigg] \\leq v_{*_{1}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215d3a5",
   "metadata": {},
   "source": [
    "We can define also a constraint for the portfolio volatility. Let:\n",
    "* $\\Sigma_{x_{t}}$  NXN  semi-positive definite rolling var-cov matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f9fce8",
   "metadata": {},
   "source": [
    "* $v_2(x_{t}) := \\sqrt{w'\\Sigma_{x{t}} w}$ <br>\n",
    "* $V_2(X) := \\bigg[v_2(x_{1}),v_2(x_{2}),...,v_2(x_{t}),...,v_2(x_{T})\\bigg]'$ <br>\n",
    "* $\\mathbb{E}_p{\\bigg(V_2(X)\\bigg)} := p'V_2(X) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb6ffd",
   "metadata": {},
   "source": [
    "\n",
    "we can set the intensity of our view $v_2*$ based on the current level of the volatility for our portfolio. <br>\n",
    "\n",
    "Assuming  a more volatile market, we can state our view as follows:\n",
    "\n",
    "*  $V:=E_{p}\\bigg[{V_2(X)}\\bigg]\\geq v_{*_{2}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4e390",
   "metadata": {},
   "source": [
    "$v_{*_{2}} \\approx 11\\%$ (yearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf1e4c",
   "metadata": {},
   "source": [
    "Defining $V_2(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_2x=(epsilon).rolling(window=wndw).std().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc89dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio volatility is SP500            0.012919\n",
      "DOW_Jones        0.012276\n",
      "Fixed_Income     0.001325\n",
      "Russell3000      0.013044\n",
      "Credit_Suisse    0.025418\n",
      "dtype: float64\n",
      "Portfolio volatility with standardiized data is SP500            1.011200\n",
      "DOW_Jones        1.012648\n",
      "Fixed_Income     1.027226\n",
      "Russell3000      1.011427\n",
      "Credit_Suisse    1.020578\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "'Portfolio volatility'\n",
    "print('Portfolio volatility is ' +str((x_restricted).std()))\n",
    "'portfolio with standardized data'\n",
    "print('Portfolio volatility with standardiized data is ' + str(v_1x.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd933fc",
   "metadata": {},
   "source": [
    "##  Entropy minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93105c10",
   "metadata": {},
   "source": [
    "$p^{post}$ = $ argmin_{q} \\ \\Bigg \\{ \\sum_{t=1}^{T}q_t(ln(q_t) - ln(p^{0}_{t})) \\Bigg \\}$ <br>\n",
    "$ Subject \\ to$<br> \n",
    "$Fq\\leq f$ <br> \n",
    "$Hq$ $=$ $h$ <br>\n",
    "We have collected all the inequality constraints in the matrix-vector pair $(F,f)$ and\n",
    "all the equality constraints in the matrix-vector pair $(H,h)$, we do not include the extra-constraint\n",
    "$\\bigg(q>0\\bigg)$ because it will be automatically satisfied. <br>\n",
    " The Lagrangian function reads: <br> <br>\n",
    " $L(q,\\lambda_{1} , \\lambda_{2})$ = $q'(ln(q)-ln(p^{0})) + \\lambda_1' (Fq-f) +  \\lambda_2'(Hq-h)$ <br> <br>\n",
    "   * $\\lambda_1$ is a row  vector with number of inequality constraint = number of rows <br>\n",
    "   * $\\lambda_2$ is a row  vector with number of equality constraint = number of rows <br>\n",
    "   * $F$ is a matrix with K rows (K=number of inequality constraint) and T columns(number of risk drivers' observations) <br>\n",
    "   * $H$ is a matrix with J rows (J=number of equality constraint) and T columns (number of risk drivers' observations) <br>\n",
    "\n",
    " the first order condition for q read: <br> <br>\n",
    " $ 0 = \\frac{dL}{dq} = ln(q) - ln(p^{0}) + 1 + F' \\lambda_1 + H'\\lambda_2$ <br> <br>\n",
    " Solving for q: <br>\n",
    " <br>\n",
    " $q(\\lambda_1,\\lambda_2) = e^{ln(p^{0}) - 1 -F'\\lambda_1 - H'\\lambda_2}$\n",
    "<br> <br>\n",
    "The solution is always positive, so we do not need the $\\bigg(q>0\\bigg)$ constraint <br> <br>\n",
    "The Lagrange dual function is defined as: <br>\n",
    "<br>\n",
    "$G(\\lambda_1,\\lambda_2) = L(q(\\lambda_1,\\lambda_2),\\lambda_1,\\lambda_2)$ <br> <br>\n",
    "The optimal Lagrange multipliers follow from the maximization of the Lagrange dual function (or the minimization of the negative Lagrange dual function): <br>\n",
    "<br>\n",
    "$ (\\lambda_1^{*},\\lambda_2^{*})$ = $ argmin \\ \\bigg \\{ -G(\\lambda_1,\\lambda_2) \\bigg \\}$ <br>\n",
    "$subject \\ to$ <br>\n",
    "$\\lambda_1\\leq 0$ <br>\n",
    "<br>\n",
    "Then with the optimal lagrange multiplayers we can define the optimal set of probabilities as: <br>\n",
    "$p$ = $q(\\lambda_1^{*},\\lambda_2^{*})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41ea4f9",
   "metadata": {},
   "source": [
    "## Defining F and H  matrix for inequality and equality constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ae087",
   "metadata": {},
   "source": [
    "H is an object used to put equality constraints.\n",
    "In this case we want that the sum of our probability is equal to one\n",
    "* $Hq=h$\n",
    "* $H = [1,1...,1]$\n",
    "* $q=[p_{1},p_{2}...p_{T}]'$\n",
    "* h=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996b9ac",
   "metadata": {},
   "source": [
    "$-F_{v_1x}q > -v_{*_{1}}$ = $F_{v_1x} < v_{*_{1}}$ <br>\n",
    "$F_{v_2x}q>v_{*_{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316bb6c",
   "metadata": {},
   "source": [
    "\n",
    "* $F= \\bigg[\\begin{matrix}\n",
    "-v_{1}(x_{t})& ... & -v_{1}(x_{T}) \\\\\n",
    "v_{2}(x_{t})& ... & v_{2}(x_{T})\\end{matrix}\\bigg]$ <br>\n",
    "* $f=\\bigg[\\begin{matrix} -v_{*_{1}} \\\\ v_{*_{2}}\\end{matrix}\\bigg]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff01e68",
   "metadata": {},
   "source": [
    "## Rolling mean and standard deviation for the asset classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16e28d",
   "metadata": {},
   "source": [
    "Insert the asset classes for which you want to insert views (in this case I am gonna select all of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c401e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_distribution=epsilon.rolling(window=wndw).mean().dropna()\n",
    "vol_distribution=epsilon.rolling(window=wndw).std().dropna()\n",
    "asset_mean_distribution=x.rolling(window=wndw).mean().dropna()\n",
    "asset_vol_distribution=x.rolling(window=wndw).std().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.Series(index=epsilon.columns)\n",
    "loc_t=pd.Series(index=epsilon.columns)\n",
    "scale_t=pd.Series(index=epsilon.columns)\n",
    "shape=pd.Series(index=epsilon.columns)\n",
    "loc_ln=pd.Series(index=epsilon.columns)\n",
    "scale_ln=pd.Series(index=epsilon.columns)\n",
    "quantile_mean=pd.Series(index=epsilon.columns)\n",
    "quantile_vol=pd.Series(index=epsilon.columns)\n",
    "v_star1=[]\n",
    "v_star2=[]\n",
    "\"Type your views on the mean, Suppose we have only views on SP500\"\n",
    "mean_views=pd.DataFrame([-0.02,-0.04,-0.05,-0.02,-0.3],index=x.columns)\n",
    "#mean_views=pd.DataFrame([-0.1],index=['SP500'])\n",
    "\"For the other asset classes we don't have views, then we are gonna stay consistent with the prior\"\n",
    "not_mean_views=pd.Series([p_0.dot(x_restricted[i]) for i in x.columns if i not in mean_views.index],index=[i for i in x.columns if i not in mean_views.index])\n",
    "'Merging the data'\n",
    "absolute_views_mean=pd.concat([mean_views,not_mean_views])\n",
    "'Type your views on volatility'\n",
    "vol_views=pd.DataFrame(list(asset_vol_distribution.min()*np.sqrt(252)),index=x.columns)\n",
    "not_vol_views=pd.Series([p_0.dot(asset_vol_distribution[i]) for i in x.columns if i not in vol_views.index],index=[i for i in x.columns if i not in vol_views.index])\n",
    "absolute_views_vol=pd.concat([vol_views,not_vol_views])\n",
    "obj_mean=absolute_views_mean[0]\n",
    "obj_vol=absolute_views_vol[0]\n",
    "for j in x.columns:\n",
    "    df.loc[j],loc_t.loc[j],scale_t.loc[j]=t.fit(asset_mean_distribution[j])\n",
    "    quantile_mean.loc[j]=t.cdf(obj_mean.loc[j]/252,df.loc[j],loc_t.loc[j],scale_t.loc[j])\n",
    "    v_star1.append(mean_distribution[j].quantile(quantile_mean.loc[j]))\n",
    "    shape.loc[j],loc_ln.loc[j],scale_ln.loc[j]=lognorm.fit(asset_vol_distribution[j])\n",
    "    quantile_vol.loc[j]=lognorm.cdf(obj_vol.loc[j]/np.sqrt(252),shape.loc[j],loc_ln.loc[j],scale_ln.loc[j])\n",
    "    v_star2.append(vol_distribution[j].quantile(quantile_vol.loc[j]))\n",
    "v_star1=pd.Series(v_star1)\n",
    "v_star2=pd.Series(v_star2)\n",
    "quantile_mean=quantile_mean.dropna()\n",
    "quantile_vol=quantile_vol.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c6531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -0.047142\n",
       "1   -0.051233\n",
       "2   -0.274532\n",
       "3   -0.045762\n",
       "4   -0.032955\n",
       "dtype: float64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_star1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a5a9f",
   "metadata": {},
   "source": [
    "# Get quantiles for the mean and the volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864def32",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_vola_view_columns=[i+'_std' for i in x.columns]\n",
    "aux=list(vol_distribution[x.columns].values)\n",
    "v_star1.index=x.columns\n",
    "v_star2.index=absolute_vola_view_columns\n",
    "v_2x=pd.DataFrame(aux,index=p_0.index,columns=absolute_vola_view_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b00b34",
   "metadata": {},
   "source": [
    "I am creating two objective functions:\n",
    "* One for the case in which we have only equality constraint\n",
    "* One for the case in which we have both\n",
    "* I am not considering the case of only inequality constraint, because the constraint on the sum of probabilities=1 must be always satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de2ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_Dual_func_eq_constr(Lmbda_vector,P_0,H_matrix,h):\n",
    "   '''Lmbda_vector is a ndarray with (k_ineq + k_eq) number of elements\n",
    "   P_0 is a series of prior probabilities with T number of elements\n",
    "   H matrix must be a dataframe K_eq(number of equality constraints) rows and T columns (T number of scenarios)\n",
    "   h is a series with equality constraints values\n",
    "   lmbda vector is an array with initial values for Lagrange multipliers\n",
    "   The function returns the objective function value to optimize '''\n",
    "   K_eq=len(h)\n",
    "   lmbda_2=Lmbda_vector[0:K_eq]\n",
    "   Lmbda_vector[K_eq:]=0\n",
    "   q=np.exp(np.log(P_0) - 1 - H_matrix.T.dot(lmbda_2))\n",
    "   Dual_func= q.T.dot(np.log(q) - np.log(P_0)) + lmbda_2.T.dot(H_matrix.dot(q)-h)\n",
    "   return - Dual_func\n",
    "\n",
    "def neg_Dual_func_constr(Lmbda_vector,P_0,F_matrix,H_matrix,f,h):\n",
    "   '''Lmbda_vector is a ndarray with (k_ineq + k_eq) number of elements\n",
    "   P_0 is a series of prior probabilities with T number of elements\n",
    "   F matrix must be a dataframe with K_ineq(number of inequality constraints) rows and T columns (T number of scenarios)\n",
    "   H matrix must be a dataframe K_eq(number of equality constraints) rows and T columns (T number of scenarios)\n",
    "   f is a series with intensity views for inequality constraints \n",
    "   h is a sereis with intensity views for equality constraints\n",
    "   lmbda vector is an array with initial values for Lagrange multipliers\n",
    "   The function returns the objective function value to optimize'''\n",
    "\n",
    "   K_eq=len(h)\n",
    "   K_ineq=len(f)\n",
    "   lmbda_1=Lmbda_vector[K_eq:K_ineq+1]\n",
    "   lmbda_2=Lmbda_vector[0:K_eq]\n",
    "   q=np.exp(np.log(P_0) - 1 - F_matrix.T.dot(lmbda_1) - H_matrix.T.dot(lmbda_2))\n",
    "   Dual_func=  q.T.dot(np.log(q) - np.log(P_0)) + lmbda_1.T.dot(F_matrix.dot(q)-f) + lmbda_2.T.dot(H_matrix.dot(q)-h)\n",
    "   return - Dual_func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6d740",
   "metadata": {},
   "source": [
    "## KKT Conditions \n",
    "* $\\lambda_1(Fq - f)=0$\n",
    "* $Hq=h$\n",
    "* $ Fq - f \\geq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba568b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda1_fun_eq(x,F_matrix,H_matrix,f,h):\n",
    "     K_ineq=len(f)\n",
    "     K_eq=len(h)\n",
    "     lmbda_1=x[K_eq:K_ineq+1] # Lagrange multipliers for inequality constraints\n",
    "     lmbda_2=x[0:K_eq]        # Lagrange multipliers for equality constraints\n",
    "     q=np.exp(np.log(p_0) - 1 - F_matrix.T.dot(lmbda_1) - H_matrix.T.dot(lmbda_2))\n",
    "     return lmbda_1*(F_matrix.dot(q)-f)\n",
    "\n",
    "def lambda2_fun_eq(x,F_matrix,H_matrix,f,h,Obj_fun):\n",
    "    function=Obj_fun\n",
    "    if function == neg_Dual_func_constr:\n",
    "        K_ineq=len(f)\n",
    "        K_eq=len(h)\n",
    "        lmbda_1=x[K_eq:K_ineq+1] # Lagrange multipliers for inequality constraints\n",
    "        lmbda_2=x[0:K_eq]        # Lagrange multipliers for equality constraints \n",
    "        q=np.exp(np.log(p_0) - 1 - F_matrix.T.dot(lmbda_1) - H_matrix.T.dot(lmbda_2))\n",
    "    else:\n",
    "         K_eq=len(h)\n",
    "         lmbda_2=x[0:K_eq]\n",
    "         q=np.exp(np.log(p_0) - 1 - H_matrix.T.dot(lmbda_2))\n",
    "    return H_matrix.dot(q)- h\n",
    "\n",
    "def ineq_cons(x,F_matrix,H_matrix,f,h):\n",
    "    K_ineq=len(f)\n",
    "    K_eq=len(h)\n",
    "    lmbda_1=x[K_eq:K_ineq+1] # Lagrange multipliers for inequality constraints\n",
    "    lmbda_2=x[0:K_eq]        # Lagrange multipliers for equality constraints \n",
    "    q=np.exp(np.log(p_0) - 1 - F_matrix.T.dot(lmbda_1) - H_matrix.T.dot(lmbda_2))\n",
    "    return F_matrix.dot(q)-f\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#absolute_views_index=absolute_views_mean_columns+ absolute_vola_view_columns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30141cea",
   "metadata": {},
   "source": [
    "We have to determine the sign of the view, the optimizer works with > constraints\n",
    "* in case of positive views we leave the original sign\n",
    "* in case of negative views we have to change the sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5efc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_view_mean_sign=['-','-','-','-','-']\n",
    "absolute_view_vol_sign=['+','+','+','+','+']\n",
    "v_1x_aux=v_1x.copy()\n",
    "v_2x_aux=v_2x.copy()\n",
    "v_star1_aux=v_star1.copy() \n",
    "v_star2_aux=v_star2.copy() \n",
    "for i in np.arange(0,len(absolute_view_mean_sign)):\n",
    "    if absolute_view_mean_sign[i]=='+':\n",
    "        continue\n",
    "    else: \n",
    "        v_1x_aux.iloc[i]=-v_1x.iloc[i]\n",
    "        v_star1_aux.iloc[i]=-v_star1.iloc[i]\n",
    "\n",
    "for i in np.arange(0,len(absolute_view_vol_sign)):\n",
    "    if absolute_view_mean_sign[i]=='+':\n",
    "        continue\n",
    "    else: \n",
    "        v_2x_aux.iloc[i]=-v_2x.iloc[i]\n",
    "        v_star2_aux.iloc[i]=-v_star2.iloc[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8789da66",
   "metadata": {},
   "source": [
    "Now we need to alternate views on mean and volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027a36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SP500            0.047142\n",
       "DOW_Jones        0.051233\n",
       "Fixed_Income     0.274532\n",
       "Russell3000      0.045762\n",
       "Credit_Suisse    0.032955\n",
       "dtype: float64"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_star1_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc004c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.0009368789093147102\n",
      "            Iterations: 12\n",
      "            Function evaluations: 49\n",
      "            Gradient evaluations: 12\n",
      "     fun: -0.0009368789093147102\n",
      "     jac: array([-1.66891550e-06, -7.64510332e-05, -1.20455300e+00])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 49\n",
      "     nit: 12\n",
      "    njev: 12\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.99885993, -0.0440573 ,  0.        ])\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.0011242275778404307\n",
      "            Iterations: 13\n",
      "            Function evaluations: 53\n",
      "            Gradient evaluations: 13\n",
      "     fun: -0.0011242275778404307\n",
      "     jac: array([-9.68677341e-07, -4.15110262e-05, -1.22598444e+00])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 53\n",
      "     nit: 13\n",
      "    njev: 13\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.99868155, -0.04767813,  0.        ])\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.04522973257315214\n",
      "            Iterations: 7\n",
      "            Function evaluations: 30\n",
      "            Gradient evaluations: 7\n",
      "     fun: -0.04522973257315214\n",
      "     jac: array([-3.62098217e-06, -2.91368924e-05, -1.10549311e+00])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 30\n",
      "     nit: 7\n",
      "    njev: 7\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-9.63817593e-01, -2.96549106e-01, -9.11439638e-16])\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.0008864416094452656\n",
      "            Iterations: 11\n",
      "            Function evaluations: 45\n",
      "            Gradient evaluations: 11\n",
      "     fun: -0.0008864416094452656\n",
      "     jac: array([-8.21059803e-06, -3.82428167e-04, -1.22710747e+00])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 45\n",
      "     nit: 11\n",
      "    njev: 11\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.99892985, -0.04275712,  0.        ])\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -0.0006097340267119956\n",
      "            Iterations: 14\n",
      "            Function evaluations: 57\n",
      "            Gradient evaluations: 14\n",
      "     fun: -0.0006097340267119956\n",
      "     jac: array([-1.98176713e-06, -1.13585993e-04, -1.36425109e+00])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 57\n",
      "     nit: 14\n",
      "    njev: 14\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-0.99944338, -0.03539285,  0.        ])\n"
     ]
    }
   ],
   "source": [
    "post_prob=pd.DataFrame(index=p_0.index)\n",
    "for i in x.columns:\n",
    "    H=pd.DataFrame(np.ones(len(p_0)),index=p_0.index,columns=['ones'])\n",
    "    h=pd.Series([1],index=H.columns)\n",
    "    F=pd.DataFrame(index=p_0.index)\n",
    "    f=pd.Series()\n",
    "    if i not in mean_views.index:\n",
    "        H=pd.concat([H,v_1x[i]],axis=1)\n",
    "        h=pd.concat[(h,pd.Series(v_star1.loc[i]))]\n",
    "    else:\n",
    "        F=pd.concat([F,v_1x_aux[i]],axis=1)\n",
    "        f=pd.concat([f,pd.Series(v_star1_aux.loc[i])])\n",
    "    if i not in vol_views.index:\n",
    "        H=pd.concat([H,v_2x[i+'_std']],axis=1)\n",
    "        h=pd.concat[(h,pd.series(v_star2.loc[i+'_std']))]\n",
    "    else:\n",
    "        F=pd.concat([F,v_2x[i+'_std']],axis=1)\n",
    "        f=pd.concat([f,pd.Series(v_star2_aux.loc[i+'_std'])])\n",
    "    K_eq=len(h)\n",
    "    K_ineq=len(f)\n",
    "    # initial guess\n",
    "    lmbda_vector_0=np.ones(K_eq+K_ineq)\n",
    "    lmbda_vector_0[K_eq:K_ineq+1]=-1\n",
    "    lmbda_2=lmbda_vector_0[0:K_eq]       # Lagrange multipliers for equality constraints\n",
    "    lmbda_1=lmbda_vector_0[K_eq:K_ineq+1]# Lagrange multipliers for inequality constraints\n",
    "    F=F.T\n",
    "    H=H.T\n",
    "    f.index=F.index\n",
    "    if (K_eq!=0) & (K_ineq!=0):\n",
    "        obj_fun= neg_Dual_func_constr\n",
    "    else:\n",
    "        obj_fun= neg_Dual_func_eq_constr\n",
    "    if K_ineq!=0:\n",
    "        obj_fun(lmbda_vector_0,time_cond_prob,F,H,f,h) # value of the negative dual function\n",
    "    else:\n",
    "        obj_fun(lmbda_vector_0,time_cond_prob,H,h) #value of the negative dual function with only equality constraints\n",
    "    if (K_ineq!=0):\n",
    "        cons =    ({'type': 'eq', 'fun': lambda1_fun_eq, 'args': (F,H,f,h)},\n",
    "           {'type': 'eq', 'fun': lambda2_fun_eq, 'args': (F,H,f,h,obj_fun)},\n",
    "           {'type': 'ineq', 'fun': ineq_cons,    'args': (F,H,f,h)})\n",
    "        arguments=(p_0,F,H,f,h)\n",
    "    else: \n",
    "        cons = ({'type': 'eq', 'fun': lambda2_fun_eq, 'args': (F,H,f,h,obj_fun)})\n",
    "        arguments=(p_0,H,h)\n",
    "    if (K_ineq!=0):\n",
    "        bnds= [(None, 0) for _ in range(K_ineq)]\n",
    "        bnds=[(None,None)]+bnds\n",
    "    else:\n",
    "        bnds= [(None,None) for _ in range(K_eq)]\n",
    "    res=spopt.minimize(obj_fun,lmbda_vector_0,method='SLSQP',args=arguments,bounds=bnds,constraints=cons,options={'maxiter':200,'disp': True})\n",
    "    Lagrangian_mltps=res.x\n",
    "    print(res)\n",
    "    lmbda_2=Lagrangian_mltps[0:K_eq]\n",
    "    lmbda_1=Lagrangian_mltps[K_eq:K_ineq+1]\n",
    "    post_prob[i]=np.exp(np.log(p_0) - 1 - F.T.dot(lmbda_1) - H.T.dot(lmbda_2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff70d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SP500            0.039978\n",
       "DOW_Jones        0.037030\n",
       "Fixed_Income     0.000491\n",
       "Russell3000      0.040841\n",
       "Credit_Suisse    0.152840\n",
       "dtype: float64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(post_prob*x_restricted**2).sum()*252"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e95d73",
   "metadata": {},
   "source": [
    "$ Fq - f \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8c074",
   "metadata": {},
   "source": [
    "## Sample Expected values and variances\n",
    " * $E(X)=\\frac{1}{N}  \\sum_{t=1}^{T} X_{t}$\n",
    " * $Var(X)= E(X^2) -E(X)^2 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae734e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP500            0.086135\n",
      "DOW_Jones        0.081408\n",
      "Fixed_Income     0.010184\n",
      "Russell3000      0.086689\n",
      "Credit_Suisse   -0.112496\n",
      "dtype: float64\n",
      "SP500            0.205083\n",
      "DOW_Jones        0.194883\n",
      "Fixed_Income     0.021027\n",
      "Russell3000      0.207063\n",
      "Credit_Suisse    0.403497\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(x_restricted.mean()*252)\n",
    "print(x_restricted.std()*np.sqrt(252))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0318f38efd34f66a233dc4c6df2ab6f566e47d01e77740aef179e188cc41a779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
